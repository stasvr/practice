{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varshavskiisd/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_input = data[0][0][0].shape[0] * data[0][0][0].shape[1] \n",
    "num_hidden = 144\n",
    "epochs = 150\n",
    "batch_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_tensor = tf.placeholder(shape=[None, num_input], dtype=tf.float32)\n",
    "\n",
    "weights_encoder = tf.Variable(tf.random_normal([num_input, num_hidden]))\n",
    "bias_encoder = tf.Variable(tf.random_normal([num_hidden]))\n",
    "\n",
    "weights_decoder = tf.Variable(tf.random_normal([num_hidden, num_input]))\n",
    "bias_decoder = tf.Variable(tf.random_normal([num_input]))\n",
    "\n",
    "def encoder(X_real):\n",
    "    return tf.nn.sigmoid(tf.add(tf.matmul(X_real, weights_encoder), bias_encoder))\n",
    "\n",
    "def decoder(X_zip):\n",
    "    return tf.nn.sigmoid(tf.add(tf.matmul(X_zip, weights_decoder), bias_decoder))\n",
    "\n",
    "hidden_tensor = encoder(input_tensor)\n",
    "\n",
    "batch_mean, batch_var = tf.nn.moments(hidden_tensor,[0])\n",
    "\n",
    "scale = tf.Variable(tf.ones([num_hidden]))\n",
    "beta = tf.Variable(tf.zeros([num_hidden]))\n",
    "\n",
    "hidden_tensor = tf.nn.batch_normalization(hidden_tensor, batch_mean, batch_var, beta, scale, 1e-3)\n",
    "\n",
    "output_tensor = decoder(hidden_tensor)\n",
    "\n",
    "loss = tf.reduce_mean(tf.pow(input_tensor - output_tensor, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-3, decay=3e-8)\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "optimizer_clipped = optimizer.apply_gradients([(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[784, 392], [392, 196], [196, 98], [98, 196], [196, 392], [392, 784]]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hidden = [392, 196, 98]\n",
    "weights_hidden = [[num_hidden[i-1], num_hidden[i]] for i in range(1, len(num_hidden))]\n",
    "weights_hidden += [[num_hidden[i], num_hidden[i-1]] for i in reversed(range(1, len(num_hidden)))]\n",
    "weights_hidden = [[num_input, num_hidden[0]]] + weights_hidden\n",
    "weights_hidden += [[num_hidden[0], num_input]]\n",
    "weights_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[392, 196, 98, 196, 392, 784]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_hidden = [num_hidden[i] for i in range(len(num_hidden))]\n",
    "bias_hidden += [num_hidden[i] for i in reversed(range(len(num_hidden)-1))]\n",
    "bias_hidden += [num_input]\n",
    "bias_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[196, 98, 196, 392]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hidden[1:] + list(reversed(num_hidden))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder:\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        num_input, num_hidden, b_norm = params['num_input'], params['num_hidden'], params['batch_normalisation']\n",
    "        \n",
    "        # Placeholders and variables used in graph\n",
    "        self._input_tensor = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "        \n",
    "        weights_hidden = [self.init_VarPair([num_hidden[i-1], num_hidden[i]]) for i in range(1, len(num_hidden))]\n",
    "        weights_hidden += [self.init_VarPair([num_hidden[i], num_hidden[i-1]]) for i in reversed(range(1, len(num_hidden)))]\n",
    "        weights_hidden = [self.init_VarPair([num_input, num_hidden[0]])] + weights_hidden\n",
    "        weights_hidden += [self.init_VarPair([num_hidden[0], num_input])]\n",
    "        \n",
    "        bias_hidden = [self.init_VarSingle(num_hidden[i]) for i in range(len(num_hidden))]\n",
    "        bias_hidden += [self.init_VarSingle(num_hidden[i]) for i in reversed(range(len(num_hidden)-1))]\n",
    "        bias_hidden += [self.init_VarSingle(num_input)]\n",
    "\n",
    "        \n",
    "        # Graph structure\n",
    "        hidden_tensor = self.coder(self._input_tensor, weights_hidden[0], bias_hidden[0])\n",
    "        for i in zip(weights_hidden[1:-1], bias_hidden[1:-1], num_hidden[1:]+list(reversed(num_hidden))[1:]):\n",
    "            if b_norm:\n",
    "                hidden_tensor = self.normalisation(self.coder(hidden_tensor, i[0], i[1]), i[2])\n",
    "            else:\n",
    "                hidden_tensor = self.coder(hidden_tensor, i[0], i[1])\n",
    "        self._output_tensor = self.coder(hidden_tensor, weights_hidden[-1:][0], bias_hidden[-1:][0])\n",
    "        \n",
    "        # Initialisation of loss funstion and optimisation method with constraits for weights changing\n",
    "        self._loss = tf.reduce_mean(tf.pow(self._input_tensor - self._output_tensor, 2))\n",
    "        self._optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-3, decay=3e-8).minimize(self._loss)\n",
    "        \n",
    "    def coder(self, X, w, b):\n",
    "        return tf.nn.sigmoid(tf.add(tf.matmul(X, w), b))\n",
    "    \n",
    "    def normalisation(self, tensor, count):\n",
    "        batch_mean, batch_var = tf.nn.moments(tensor,[0])\n",
    "        scale = tf.Variable(tf.ones([count]))\n",
    "        beta = tf.Variable(tf.zeros([count]))\n",
    "        return tf.nn.batch_normalization(tensor, batch_mean, batch_var, beta, scale, 1e-3)\n",
    "    \n",
    "    def init_VarPair(self, pair):\n",
    "        return tf.Variable(tf.random_normal([pair[0], pair[1]]))\n",
    "\n",
    "    def init_VarSingle(self, value):\n",
    "        return tf.Variable(tf.random_normal([value]))\n",
    "    \n",
    "    @property\n",
    "    def out(self):\n",
    "        return self._output_tensor\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self._loss\n",
    "    \n",
    "    @property\n",
    "    def optimize(self):\n",
    "        return self._optimizer\n",
    "        \n",
    "class Process:\n",
    "    \n",
    "    def __init__(self, data, params):\n",
    "        self._epochs, self._batch_size = params['epochs'], params['batch_size']\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        self._sess = tf.Session()\n",
    "        self._sess.run(init)\n",
    "        \n",
    "        self._data, last, self._iterations = list(), 0, int(data.shape[0] / self._batch_size)\n",
    "        for i in range(self._iterations):\n",
    "            if i != 0:\n",
    "                self._data.append(data[last:i*batch_size])\n",
    "                last = i * batch_size\n",
    "    \n",
    "    def run(self, graph):\n",
    "        for i in range(self._epochs):\n",
    "            tmp = iter(self._data)\n",
    "            for j in range(self._iterations-1):\n",
    "                batch = next(tmp)\n",
    "                _, l = self._sess.run([graph.optimize, graph.loss], feed_dict={graph._input_tensor : batch})\n",
    "            if i % 5 == 0:\n",
    "                print('Epoch:', i, 'Loss:', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "ae = AutoEncoder({\n",
    "    'num_input' : 784,\n",
    "    'num_hidden' : [392, 196, 98, 49],\n",
    "    'batch_normalisation' : True\n",
    "})\n",
    "buff = MinMaxScaler().fit_transform(data[0][0].reshape(data[0][0].shape[0], num_input))\n",
    "process = Process(buff, {'epochs' : 10, 'batch_size' : 250})\n",
    "process.run(ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.244661\n",
      "Epoch: 1 Loss: 0.0803636\n",
      "Epoch: 2 Loss: 0.0643819\n",
      "Epoch: 3 Loss: 0.0521874\n",
      "Epoch: 4 Loss: 0.0492202\n",
      "Epoch: 5 Loss: 0.04733\n",
      "Epoch: 6 Loss: 0.045788\n",
      "Epoch: 7 Loss: 0.0443011\n",
      "Epoch: 8 Loss: 0.0429985\n",
      "Epoch: 9 Loss: 0.041832\n",
      "Epoch: 10 Loss: 0.040826\n",
      "Epoch: 11 Loss: 0.0399039\n",
      "Epoch: 12 Loss: 0.0389975\n",
      "Epoch: 13 Loss: 0.0381926\n",
      "Epoch: 14 Loss: 0.0374306\n",
      "Epoch: 15 Loss: 0.0367219\n",
      "Epoch: 16 Loss: 0.0360719\n",
      "Epoch: 17 Loss: 0.0354679\n",
      "Epoch: 18 Loss: 0.0349051\n",
      "Epoch: 19 Loss: 0.0343681\n",
      "Epoch: 20 Loss: 0.0338098\n",
      "Epoch: 21 Loss: 0.0332831\n",
      "Epoch: 22 Loss: 0.032812\n",
      "Epoch: 23 Loss: 0.0323618\n",
      "Epoch: 24 Loss: 0.0319133\n",
      "Epoch: 25 Loss: 0.0315004\n",
      "Epoch: 26 Loss: 0.0311068\n",
      "Epoch: 27 Loss: 0.0307289\n",
      "Epoch: 28 Loss: 0.0303667\n",
      "Epoch: 29 Loss: 0.0300118\n",
      "Epoch: 30 Loss: 0.0296649\n",
      "Epoch: 31 Loss: 0.0293195\n",
      "Epoch: 32 Loss: 0.0289936\n",
      "Epoch: 33 Loss: 0.0286815\n",
      "Epoch: 34 Loss: 0.0283851\n",
      "Epoch: 35 Loss: 0.028104\n",
      "Epoch: 36 Loss: 0.0278405\n",
      "Epoch: 37 Loss: 0.0275873\n",
      "Epoch: 38 Loss: 0.027334\n",
      "Epoch: 39 Loss: 0.0270649\n",
      "Epoch: 40 Loss: 0.0268429\n",
      "Epoch: 41 Loss: 0.0266177\n",
      "Epoch: 42 Loss: 0.026402\n",
      "Epoch: 43 Loss: 0.0261982\n",
      "Epoch: 44 Loss: 0.0259976\n",
      "Epoch: 45 Loss: 0.0257991\n",
      "Epoch: 46 Loss: 0.0256065\n",
      "Epoch: 47 Loss: 0.0254199\n",
      "Epoch: 48 Loss: 0.0252397\n",
      "Epoch: 49 Loss: 0.0250657\n",
      "Epoch: 50 Loss: 0.0248977\n",
      "Epoch: 51 Loss: 0.0247355\n",
      "Epoch: 52 Loss: 0.0245792\n",
      "Epoch: 53 Loss: 0.0244264\n",
      "Epoch: 54 Loss: 0.0242779\n",
      "Epoch: 55 Loss: 0.0241346\n",
      "Epoch: 56 Loss: 0.0239945\n",
      "Epoch: 57 Loss: 0.0238572\n",
      "Epoch: 58 Loss: 0.0237215\n",
      "Epoch: 59 Loss: 0.0235871\n",
      "Epoch: 60 Loss: 0.0234563\n",
      "Epoch: 61 Loss: 0.0233166\n",
      "Epoch: 62 Loss: 0.0231813\n",
      "Epoch: 63 Loss: 0.0230454\n",
      "Epoch: 64 Loss: 0.022915\n",
      "Epoch: 65 Loss: 0.0227779\n",
      "Epoch: 66 Loss: 0.0226497\n",
      "Epoch: 67 Loss: 0.0225315\n",
      "Epoch: 68 Loss: 0.0224143\n",
      "Epoch: 69 Loss: 0.0222973\n",
      "Epoch: 70 Loss: 0.0221802\n",
      "Epoch: 71 Loss: 0.0220648\n",
      "Epoch: 72 Loss: 0.0219527\n",
      "Epoch: 73 Loss: 0.021841\n",
      "Epoch: 74 Loss: 0.02173\n",
      "Epoch: 75 Loss: 0.0216174\n",
      "Epoch: 76 Loss: 0.0215049\n",
      "Epoch: 77 Loss: 0.0213913\n",
      "Epoch: 78 Loss: 0.0212792\n",
      "Epoch: 79 Loss: 0.0211702\n",
      "Epoch: 80 Loss: 0.0210647\n",
      "Epoch: 81 Loss: 0.0209623\n",
      "Epoch: 82 Loss: 0.0208623\n",
      "Epoch: 83 Loss: 0.0207643\n",
      "Epoch: 84 Loss: 0.020669\n",
      "Epoch: 85 Loss: 0.0205757\n",
      "Epoch: 86 Loss: 0.0204843\n",
      "Epoch: 87 Loss: 0.0203959\n",
      "Epoch: 88 Loss: 0.0203094\n",
      "Epoch: 89 Loss: 0.0202219\n",
      "Epoch: 90 Loss: 0.0201352\n",
      "Epoch: 91 Loss: 0.0200496\n",
      "Epoch: 92 Loss: 0.0199657\n",
      "Epoch: 93 Loss: 0.0198836\n",
      "Epoch: 94 Loss: 0.0198023\n",
      "Epoch: 95 Loss: 0.0197222\n",
      "Epoch: 96 Loss: 0.019643\n",
      "Epoch: 97 Loss: 0.0195652\n",
      "Epoch: 98 Loss: 0.0194892\n",
      "Epoch: 99 Loss: 0.0194145\n",
      "Epoch: 100 Loss: 0.0193403\n",
      "Epoch: 101 Loss: 0.0192651\n",
      "Epoch: 102 Loss: 0.0191822\n",
      "Epoch: 103 Loss: 0.0190971\n",
      "Epoch: 104 Loss: 0.0190116\n",
      "Epoch: 105 Loss: 0.0189258\n",
      "Epoch: 106 Loss: 0.0188409\n",
      "Epoch: 107 Loss: 0.0187564\n",
      "Epoch: 108 Loss: 0.0186726\n",
      "Epoch: 109 Loss: 0.0185902\n",
      "Epoch: 110 Loss: 0.0185089\n",
      "Epoch: 111 Loss: 0.0184288\n",
      "Epoch: 112 Loss: 0.0183457\n",
      "Epoch: 113 Loss: 0.0182659\n",
      "Epoch: 114 Loss: 0.0181881\n",
      "Epoch: 115 Loss: 0.0181123\n",
      "Epoch: 116 Loss: 0.0180377\n",
      "Epoch: 117 Loss: 0.0179645\n",
      "Epoch: 118 Loss: 0.0178926\n",
      "Epoch: 119 Loss: 0.0178219\n",
      "Epoch: 120 Loss: 0.0177521\n",
      "Epoch: 121 Loss: 0.0176824\n",
      "Epoch: 122 Loss: 0.0176097\n",
      "Epoch: 123 Loss: 0.0175411\n",
      "Epoch: 124 Loss: 0.0174739\n",
      "Epoch: 125 Loss: 0.0174076\n",
      "Epoch: 126 Loss: 0.0173425\n",
      "Epoch: 127 Loss: 0.0172785\n",
      "Epoch: 128 Loss: 0.0172154\n",
      "Epoch: 129 Loss: 0.0171531\n",
      "Epoch: 130 Loss: 0.0170918\n",
      "Epoch: 131 Loss: 0.0170318\n",
      "Epoch: 132 Loss: 0.0169724\n",
      "Epoch: 133 Loss: 0.016913\n",
      "Epoch: 134 Loss: 0.0168537\n",
      "Epoch: 135 Loss: 0.0167947\n",
      "Epoch: 136 Loss: 0.016736\n",
      "Epoch: 137 Loss: 0.0166773\n",
      "Epoch: 138 Loss: 0.0166189\n",
      "Epoch: 139 Loss: 0.0165605\n",
      "Epoch: 140 Loss: 0.0165023\n",
      "Epoch: 141 Loss: 0.0164438\n",
      "Epoch: 142 Loss: 0.0163855\n",
      "Epoch: 143 Loss: 0.0163271\n",
      "Epoch: 144 Loss: 0.0162689\n",
      "Epoch: 145 Loss: 0.0162116\n",
      "Epoch: 146 Loss: 0.0161578\n",
      "Epoch: 147 Loss: 0.0161011\n",
      "Epoch: 148 Loss: 0.0160438\n",
      "Epoch: 149 Loss: 0.0159866\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "ae = AutoEncoder({\n",
    "    'num_input' : 784,\n",
    "    'num_hidden' : [392, 196],\n",
    "    'input' : tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "})\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Small preprocess\n",
    "buff = MinMaxScaler().fit_transform(data[0][0].reshape(data[0][0].shape[0], num_input))\n",
    "X, last, iterations = list(), 0, int(data[0][0].shape[0] / batch_size)\n",
    "for i in range(int(data[0][0].shape[0] / batch_size)):\n",
    "    if i != 0:\n",
    "        X.append(buff[last:i*batch_size])\n",
    "        last = i * batch_size\n",
    "\n",
    "# Training\n",
    "for i in range(epochs):\n",
    "    tmp = iter(X)\n",
    "    for j in range(iterations-1):\n",
    "        batch = next(tmp)\n",
    "        _, l = sess.run([ae.optimize, ae.loss], feed_dict={ae._input_tensor : batch})\n",
    "    print('Epoch:', i, 'Loss:', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Small preprocess\n",
    "buff = MinMaxScaler().fit_transform(data[0][0].reshape(data[0][0].shape[0], num_input))\n",
    "X, last, iterations = list(), 0, int(data[0][0].shape[0] / batch_size)\n",
    "for i in range(int(data[0][0].shape[0] / batch_size)):\n",
    "    if i != 0:\n",
    "        X.append(buff[last:i*batch_size])\n",
    "        last = i * batch_size\n",
    "          \n",
    "# Training            \n",
    "for i in range(epochs):\n",
    "    tmp = iter(X)\n",
    "    for j in range(iterations-1):\n",
    "        batch = next(tmp)\n",
    "        _, l = sess.run([optimizer_clipped, loss], feed_dict={input_tensor : batch})\n",
    "    print('Epoch:', i, 'Loss:', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efe603790b8>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAJCCAYAAADdrPONAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nta/eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe32697d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand = np.random.choice(2, size=[batch_size, num_input], p=[0.95, 0.05])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2,2,1)\n",
    "g = sess.run(ae._output_tensor, feed_dict={ ae._input_tensor : (X[5] + rand)})\n",
    "plt.imshow(g[0].reshape([28, 28]))\n",
    "plt.subplot(2,2,2)\n",
    "g = sess.run(ae._output_tensor, feed_dict={ ae._input_tensor : X[5]})\n",
    "plt.imshow(g[0].reshape([28, 28]))\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow((X[5][0] + rand[0]).reshape([28, 28]))\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow((X[5][0]).reshape([28, 28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
